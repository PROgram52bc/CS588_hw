\documentclass[12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage[ruled,vlined,linesnumbered,noend]{algorithm2e}
\usepackage{dsfont}
\usepackage{fancyhdr}
\usepackage{amsthm}

\newcommand{\hw}{CS 588: Homework 6}
\newcommand{\me}{David Deng}

\pagestyle{fancy}

\fancyhf{} % clear all header/footer fields

% Optional thin rule above the footer (comment out if you don't want it)
\renewcommand{\footrulewidth}{0.4pt}

\fancyfoot[L]{\hw}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\me}

% Ensure chapter/section opening pages also use the same footer (for classes that use 'plain' style)
\fancypagestyle{plain}{%
\fancyfoot[L]{\hw}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\me}
}

\begin{document}
\section*{Exercise 22.1}

\begin{enumerate}
    \item

    \begin{proof}
    
    We first show that the numerator $\langle x, Mx \rangle$ is equivalent to
    $\langle \mathds{1}_S, L\mathds{1}_S \rangle$:

    \[\begin{array}{lll}
    & {\langle x, Mx \rangle} \\
    = & {\langle x, (I-Q)x \rangle} \\
    = & {\langle x, (I-D^{-1/2}AD^{-1/2})x \rangle} \\
    = & {\langle D^{1/2}\mathds{1}_{S}, (I-D^{-1/2}AD^{-1/2})D^{1/2}\mathds{1}_{S} \rangle} \\
    = & {\langle D^{1/2}\mathds{1}_{S}, D^{1/2}\mathds{1}_{S}-D^{-1/2}AD^{-1/2}D^{1/2}\mathds{1}_{S} \rangle} \\
    = & {\langle D^{1/2}\mathds{1}_{S}, D^{1/2}\mathds{1}_{S}-D^{-1/2}A\mathds{1}_{S} \rangle} \\
    \overset{(1)}{=} & {\langle D^{1/2}\mathds{1}_{S}, D^{1/2}\mathds{1}_{S}-D^{1/2}D^{-1}A\mathds{1}_{S} \rangle} \\
    \overset{(2)}{=} & {\langle \mathds{1}_{S}, D\mathds{1}_{S}-DD^{-1}A\mathds{1}_{S} \rangle} \\
    = & {\langle \mathds{1}_{S}, D\mathds{1}_{S}-A\mathds{1}_{S} \rangle} \\
    = & {\langle \mathds{1}_{S}, (D - A)\mathds{1}_{S} \rangle} \\
    = & {\langle \mathds{1}_{S}, L\mathds{1}_{S} \rangle} \\
    \end{array}\]

    where (1) and (2) are because $D$ is a diagonal matrix.

    We then show that the denominator $\langle x,x\rangle$ is equivalent to
    $\langle \mathds{1}_S, D\mathds{1}_S \rangle$:

    \[\begin{array}{lll}
    & \langle x,x \rangle \\
    = & {\langle D^{1/2}\mathds{1}_{S},D^{1/2}\mathds{1}_{S} \rangle} \\
    \overset{(3)}{=} & {\langle \mathds{1}_{S},D\mathds{1}_{S} \rangle}
    \end{array}\]

    where (3) is because $D$ is a diagonal matrix.
    
    Thus, we have $$\frac{\langle x, Mx \rangle}{\langle x,x \rangle} =
    \frac{\langle \mathds{1}_{S}, L\mathds{1}_{S} \rangle}{\langle
    \mathds{1}_{S},D\mathds{1}_{S} \rangle}$$
    
    Similarly, we can show $$\frac{\langle y, My \rangle}{\langle y,y \rangle} =
    \frac{\langle \mathds{1}_{\bar{S}}, L\mathds{1}_{\bar{S}} \rangle}{\langle
    \mathds{1}_{\bar{S}},D\mathds{1}_{\bar{S}} \rangle}$$ using the same
    derivation.
    
    We also have $\langle x,y\rangle = 0$ because $S \cap \bar{S} = \emptyset$,
    thus $(\mathds{1}_S)_i (\mathds{1}_{\bar{S}})_i = 0$ for all $i$. Therefore,
    
    $$ \langle x,y\rangle = \sum_{i \in V} x_i y_i = \sum_{i \in V} D^{1/2}_{ii}
    (\mathds{1}_S)_i D^{1/2}_{ii} (\mathds{1}_{\bar{S}})_i = \sum_{i} 0 = 0$$
    \end{proof}

    \item 
    \begin{proof}
    Let \[\begin{array}{lll}
    z &=& \alpha x + \beta y \\
    & = & \alpha D^{1/2}\mathds{1}_{S} + \beta D^{1/2}\mathds{1}_{\bar{S}} \\
    & = & D^{1/2}(\alpha \mathds{1}_{S} + \beta \mathds{1}_{\bar{S}}) \\
    & = & D^{1/2}u
    \end{array}\]
    where $u = \alpha \mathds{1}_{S} + \beta \mathds{1}_{\bar{S}}$.

    We want to show that $$\frac{\langle z,Mz \rangle}{\langle z,z \rangle} \le 2 \Psi(G)$$

    For the numerator, we have $$\begin{array}{lll}
    \langle z,Mz \rangle &=& \langle D^{1/2}u, MD^{1/2}u \rangle \\
    &=& \langle D^{1/2}u, D^{-1/2}LD^{-1/2}D^{1/2}u \rangle \\
    &=& \langle D^{1/2}u, D^{-1/2}Lu \rangle \\
    &=& \langle u, Lu \rangle \\
    &\overset{(1)}{=}& \sum_{e = \{a,b\} \in E} (u_a - u_b)^2 \\
    &\overset{(2)}{=}& \sum_{e = \{a,b\} \in \delta(S)} (\alpha - \beta)^2 \\
    &=& \vert \delta(S) \vert \cdot(\alpha - \beta)^2 \\
    \end{array}$$

    where (1) is by the definition of $L$ described in section 21.1 in the
    textbook, and (2) is because all $u_a - u_b = 0$ if $a,b \in S$ or $a,b \in
    \bar{S}$, and $(u_a - u_b)^2 = (u_b - u_a)^2 = (\alpha - \beta)^2$ for some $a \in S$ and $b \in \bar{S}$ WLOG.

    For the denominator, we have $$\begin{array}{lll}
        \langle z,z \rangle &=& \alpha^2 \langle x,x \rangle + \beta^2 \langle y,y \rangle + 2\alpha\beta \langle x,y \rangle \\
        &\overset{(2)}{=}& \alpha^2 \langle x,x \rangle + \beta^2 \langle y,y \rangle \\
        &=& \alpha^2 \langle \mathds{1}_{S}, D\mathds{1}_{S} \rangle + \beta^2 \langle \mathds{1}_{\bar{S}}, D\mathds{1}_{\bar{S}} \rangle \\
        &=& \alpha^2 vol(S) + \beta^2 vol(\bar{S}) \\
        &\overset{(3)}{\ge}& \alpha^2 vol(S) + \beta^2 vol(S) \\
    \end{array}$$
    where (2) is because $\langle x,y\rangle = 0$, (3) is because $vol(S) \le vol(G)/2 \le vol(\bar{S})$.

    Overall, we have $$\begin{array}{lll}
    \frac{\langle z,Mz \rangle}{\langle z,z \rangle} 
    &=& \frac{\vert \delta(S) \vert \cdot (\alpha - \beta)^2}{\alpha^2 vol(S) + \beta^2 vol(\bar{S}) } \\
    &\le& \frac{\vert \delta(S) \vert \cdot (\alpha - \beta)^2}{\alpha^2 vol(S) + \beta^2 vol(S) } \\
    &\overset{(3)}{=}& \Psi(S) \frac{(\alpha - \beta)^2}{\alpha^2 + \beta^2} \\
    &\overset{(4)}{=}& \Psi(G) \frac{(\alpha - \beta)^2}{\alpha^2 + \beta^2} \\
    &\overset{(5)}{\le}& 2\Psi(G)
    \end{array}$$

    where (3) is by the definition of conductance, (4) is by assumption, (5) is
    because we can show $\frac{(\alpha - \beta)^2}{2(\alpha^2 + \beta^2)} \le 1$ by
    multiplying both sides by a positive $2(\alpha^2 + \beta^2)$. We reduce the goal to
    $$(\alpha-\beta)^2 \le 2(\alpha^2 + \beta^2)$$
    $$\alpha^2 - 2 \alpha\beta + \beta^2 \le 2(\alpha^2 + \beta^2)$$
    $$- 2 \alpha\beta \le \alpha^2 + \beta^2$$
    $$0 \le \alpha^2 +2\alpha\beta+ \beta^2$$
    $$0 \le (\alpha + \beta)^2 $$
    \end{proof}

    \item 
    \begin{proof}
        We have \[\begin{array}{lll}
            LHS &=& \alpha\langle \sqrt{d},x \rangle + \beta\langle \sqrt{d},y \rangle \\
            &\overset{(1)}{=}& \alpha\langle \sqrt{d},D^{1/2}\mathds{1}_{S} \rangle + \beta\langle \sqrt{d},D^{1/2}\mathds{1}_{\bar{S}} \rangle \\
            &\overset{(2)}{=}& \alpha\langle \mathds{1}_{S}, D\mathds{1}_{S} \rangle + \beta\langle \mathds{1}_{\bar{S}}, D\mathds{1}_{\bar{S}} \rangle \\
            &\overset{(3)}{=}& \alpha \cdot vol(S) + \beta \cdot vol(\bar{S}) \\
        \end{array}\]
        where (1) is by the definition of $x$ and $y$, (2) is because $D$ is a diagonal matrix, (3) is by the definition of volume.

        We can then choose $\alpha = -vol(\bar{S})$ and $\beta = vol(S)$ to get 0 as a result.
    \end{proof}

    \item
    \begin{proof}
        Since $\sqrt{d}$ is the first eigenvector $u_1$ of $M$ corresponding to the eigenvalue of $0$ ($1$ for $Q$), the second eigenvector $\lambda_2$ can be obtained by optimizing the quotient 

        $$\lambda_2 = \min_{v \perp \sqrt{d}} = \frac{\langle v,Mv
        \rangle}{\langle v,v \rangle}$$

        From the previous question, we know that $z = \alpha x + \beta y$ is a
        possible candidate for $v$ that satisfies $v \perp \sqrt{d}$.

        Thus, it must be the case that $\lambda_2 \le \frac{\langle z,Mz
        \rangle}{\langle v,v \rangle}$. Since we have $ \frac{\langle z,Mz
        \rangle}{\langle v,v \rangle} \le 2 \Psi(G)$, we have $\lambda_2 \le 2
        \Psi(G)$.

    \end{proof}

\end{enumerate}


\newpage
\section*{Exercise 23.6}

\begin{enumerate}

    \item 
    \begin{proof}
    
    By contradiction, suppose there is some $S$ with small cut s.t. $\vert
    \delta(S)\vert < \vert S\vert / 6$, then by the claim, for all $T$ s.t.
    $\vert T\vert = k/6 = \vert S\vert/6$, there is a vertex $v$ in $S$ matching to a
    vertex outside of $S \cup T$.

    Then we can construct an instance of $T$ with size $\vert S\vert/6$ that
    contains all vertices incident to $\delta(S)$. We know this is possible
    because $\vert \delta(S) \vert < \vert S \vert/6$ by assumption.

    Now, for this particular $T$ we constructed, there is no vertex in $S$
    matching to a vertex outside of $S \cup T$, which is a contradiction.

    Therefore, the claim above must imply that $\vert \delta(S) \vert \ge \vert
    S \vert /6$ for all $S$ with $\vert S\vert \le n/2$.
    \end{proof}

    \item 
    \begin{proof}
        

    By definition of conductance, $$\Psi(G) = \min_{S \subsetneq G, vol(S) \le
    vol(G)/2} \frac{w(\delta(S))}{vol(S)}$$

    Since the graph has $d$ perfect matchings, each vertex has a constant degree
    of $d$, and for any subset $S $, $vol(S) = \vert S \vert \cdot d$.

    By the claim, we know that for every such set $S$, $\vert \delta(S) \vert
    \ge \vert S\vert / 6$, thus $w(\delta(S)) \ge \vert S\vert /6$.

    Therefore, the conductance of the graph is $\Psi(G) = \frac{\vert
    \delta(S)\vert}{vol(S)} \ge \frac{\vert S\vert / 6}{\vert S\vert \cdot d} =
    \frac{1}{6d}$
    \end{proof}

    \item 
    \begin{proof}
    Let $M$ denote the number of matchings in the graph $G$. We have $M =
    (n-1) \times (n-3) \times (n-5) \times \cdots \times 1$.

    Let $m$ be a particular matching among the $M$ matchings.

    The algorithm finds $m$ if and only if for each of the $n/2$ steps, it picks
    the correct vertex, which has a probability of $\Pr[\text{find } m] =
    \frac{1}{n-1} \times \frac{1}{n-3} \times \frac{1}{n-5} \times \cdots \times
    \frac{1}{1} = \frac{1}{M}$. Therefore, the algorithm generates a uniformly
    random perfect matching.
    \end{proof}

    \item 
    \begin{proof}
    Let $X_i$ denote the event that the $i^{th}$ sampled matching ended up in $S \cup T$. Then 
    $$\Pr[X_1] = \frac{7k/6 -1}{n-1}$$
    $$\Pr[X_2 \vert X_1] = \frac{7k/6 -3}{n-3}$$
    $$\Pr[X_i \vert X_1, X_2, \cdots, X_{i-1}] = \frac{7k/6 -(2i-1)}{n-(2i-1)}$$
    
    In the worst case, we can have a matching of size $k/2$ where all $k/2$
    vertices in $s$ are matched to the other $k/2$ vertices in $S$.

    The probability of this happening is \[\begin{array}{lll}
    \Pr[X_{k/2}] &=& \prod_{i=1}^{k/2}{\Pr[X_i \vert X_1, X_2, \cdots, X_{i-1}]} \\
    &=& \prod_{i=1}^{k/2}{\frac{7k/6 -(2i-1)}{n-(2i-1)}} \\
    &\le& \prod_{i=1}^{k/2}{\frac{7k/6}{n}} \\
    &=& \bigl(\frac{7k}{6n}\bigr)^{k/2}
    \end{array}\]

    By union bound, the probability that this occurs to all $d$ matching is
    \[ \begin{array}{lll}
        $$\begin{array}{lll}
        \Pr[X_{k/2}^d] &=& \Pr[X_{k/2}]^d \\
        &\le& \bigl(\frac{7k}{6n}\bigr)^{kd/2} \\
        &=& \bigl(\frac{7}{6}\bigr)^{kd/2} \cdot \bigl(\frac{k}{n}\bigr)^{kd/2} \\
        &\overset{(1)}{\le}& \bigl(\frac{7}{6}\bigr)^{kd/2} \cdot {{n}\choose{k}}^{-d/2} \\
        &=& \bigl(\frac{7}{6}\bigr)^{kd/2} \cdot {{n}\choose{k}}^{-d/3} \cdot {{n}\choose{k}}^{-d/6} \\
        &\le& \bigl(\frac{7}{6}\bigr)^{kd/2} \cdot {\bigl(\frac{k}{n}\bigr)}^{kd/3} \cdot {{n}\choose{k}}^{-d/6} \\
        &\overset{(2)}{\le}& {{n}\choose{k}}^{-d/6} \\
        \end{array}$$
    \end{array} \]

    where (1) is because $(\frac{k}{n})^k \le {n \choose k}^{-1}$, (2) is
    because $\left(\frac{7}{6}\right)^{kd/2} \cdot \left(\frac{k}{n}\right)^{kd/3} \le 1$

    \end{proof}
    \item 
    \begin{proof}
        We have 
        \[\begin{array}{lll}
        \Pr[\text{fail}] &\le& \sum_{k=1}^{n/2} \binom{n}{k} \cdot \binom{n-k}{k/6} \cdot \binom{n}{k}^{-d/6} \\
        &=& \sum_{k=1}^{n/2} \binom{n}{k}^{1-d/6} \cdot \binom{n-k}{k/6} \\ 
        &\overset{(1)}{\le}& \sum_{k=1}^{n/2} \binom{n}{k}^{-1} \cdot \binom{n-k}{k/6} \\
        &=& \sum_{k=1}^{n/2} \frac{\binom{n-k}{k/6}}{\binom{n}{k}} \\
        &\le& \sum_{k=1}^{n/2} \frac{(\frac{6e(n-k)}{k})^{k/6}}{(\frac{n}{k})^k} \\
        &=& \sum_{k=1}^{n/2} \frac{(6e)^{k/6}(n-k)^{k/6}k^k}{(k^{k/6})n^k} \\
        &\le& \sum_{k=1}^{n/2} \frac{(6e)^{k/6}k^{5k/6}}{n^{5k/6}} \\
        &=& \sum_{k=1}^{n/2} \bigl((6e)^{1/5}\frac{k}{n}\bigr)^{5k/6} \\
        \end{array}\]
        where we pick $d = 12$ in (1).

        Let $C = (6e)^{1/5} \approx 1.748$,

        For small $k \approx 1$, the inner term is
        $(C\frac{k}{n})^{5k/6} \approx (1.748 \times \frac{1}{n})^{5/6} \approx
        0$ as $n \to \infty$.  

        For big $k \approx n/2$, the inner term is $(C\frac{k}{n})^{5k/6}
        \approx (1.748 \times \frac{1}{2})^{5n/12} \approx (0.87)^{5n/12}
        \approx 0$ as $n \to \infty$.

        Therefore, we have $\Pr[\text{fail}] \approx 0$ as $n \to \infty$.
        Thus the claim is true w.h.p.

        \vspace{-1.5em}
    
    \end{proof}

    \item 
    \begin{proof}
        We construct an expander using the following steps:
        \begin{enumerate}
            \item Construct a random graph by forming d=12 perfect matchings, and
            \item Add self loops to each vertex.
        \end{enumerate}

        By previous steps, this algorithm gives us a graph with constant
        conductance of $1/6d = 1/72$.

        By Cheeger's inequality, $\lambda_2 \ge \frac{\Psi(G)^2}{2} = \frac{1}{10386}$.

        After adding self-loops, we have $\lambda_2' = \lambda_2/2 = \frac{1}{20772}$

        In the worst case, the random-walk matrix has smallest eigenvalue $\mu_n
        = -1$, which means $\lambda_n = 1 - \mu_n = 2$. Adding self-loop gives us
        $\lambda_n' = (\lambda_n)/2 = 1$.

        Thus, the graph has a constant spectral gap of $$\gamma = 1 -
        \min{(\lambda_2', 2-\lambda_n')} = 1/20772$$ and thus is an expander.

    \end{proof}

\end{enumerate}

\newpage
\section*{Problem 4.8}

\begin{enumerate}
    \item 
    \begin{proof}

        $G$ is $\vert F\vert$-regular because for each vertex $(a,b)$, there is
        an edge to $(c,d)$ iff $ac = b+d$. We observe that for a particular $c$,
        there is only one $d$ that satisfies the equation. There are exactly
        $\vert F\vert$ choices of $c$. Thus for each vertex, there are exactly
        $\vert F\vert$ edges. Thus $G$ is $\vert F\vert$-regular.

        \vspace{2em}

        To prove that $\lambda(G) \le \frac{1}{\sqrt{\vert F\vert}}$, we
        consider the power graph of $G$, $G^2$, and show that $\lambda(G^2) \le
        \frac{1}{\vert F\vert}$.

        We analyze a pair of vertex $(a,b)$ and $(e,f)$ in $G^2 = G \times G$,
        where $G = (V,E)$. There must be some vertex $(c,d)$ in $G$ such that
        $$\{(a,b), (c,d)\}, \{(c,d), (e,f)\} \in E$$

        By the definition of $G$, we know that $ac = b + d$ and $ce = d + f$. We
        derive from the above equations that $c(a-e) = b-f$.

        Let $a,b,e,f$ be fixed coordinates, we solve for $c$ and have the
        following scenarios:

        \begin{enumerate}

            \item $a-e = 0$, $b-f \ne 0$: In this case there is no solution,
            where the adjacency matrix $A_{((a,b),(e,f))}$ of $G^2$ is $0$.

            \item $a-e = 0$, $b-f = 0$: In this case there are $\vert F\vert$
            possible solutions for $c$, and thus $A_{((a,b),(e,f))}$ is $\vert
            F\vert$.

            \item $a-e \ne 0$, $b-f \ne 0$: In this case there is exactly one
            solution $c = \frac{b-f}{a-e}$, and $A_{((a,b),(e,f))}$ is $1$.

        \end{enumerate}

        Observe that for all entries $A_{((a,b),(e,f))} \ne 1$, we always have
        $a = e$. If we order all vertices in the adjacency matrix by their
        coordinates, entries with $a = e$ correspond to squares of width $\vert F\vert$ in $A$ along its diagonal. 

        A is of the form 
        \[
            A = \begin{bmatrix}
            A_{1} & \cdots & 1 \\
            \vdots & \ddots & \vdots \\
            1 & \cdots & A_{\vert F \vert}
            \end{bmatrix}
        \]

        where $A_i$ is a square matrix of size $\vert F\vert \times \vert
        F\vert$ and whose entries contains either $0$ or $\vert F\vert$.

        Since $G$ is $\vert F\vert$-regular, $G^2$ has degree $\vert F\vert^2$.
        We can normalize $A$, retrieving $R = AD^{-1} = A/\vert F\vert^2$.

        R is of the form 
        \[
            R = \begin{bmatrix}
            R_{1} & \cdots & 1/\vert F\vert^2\\
            \vdots & \ddots & \vdots \\
            1/\vert F\vert^2 & \cdots & R_{\vert F \vert}
            \end{bmatrix}
        \]

        where $R_i$ is a square matrix of size $\vert F\vert \times \vert
        F\vert$ and whose entries contains either $0$ (off-diagonal) or $1/\vert F\vert$ (diagonal).

        By theorem 19.11, the random walk matrix of $G^2$, $R = AD^{-1}$, has
        the largest eigenvalue $\mu_1 = 1$. We also know that for any random
        walk matrix, $R^T \mathds{1} = \mathds{1}$. Since $G^2$ is regular, $R$
        is symmetric, thus $R \mathds{1} = \mathds{1}$, and $\mathds{1}$
        corresponds to the stationary distribution of $G^2$.
        
        We perform eigen-decomposition on the symmetric matrix $R$ following
        theorem 21.5: Let $S = (\mathds{1} \otimes \mathds{1})/\vert F\vert^2$,
        then $R = S + E$, where $E = R - S$ is the non-stationary portion of $R$
        in the form of

        \[
            E = R - S = 
            \begin{bmatrix}
            B_1 & \cdots & 0 \\
            \vdots & \ddots & \vdots \\
            0 & \cdots & B_{\vert F \vert}
            \end{bmatrix} =
            \begin{bmatrix}
            R_{1} - \frac{1}{\vert F\vert^2} & \cdots & 0 \\
            \vdots & \ddots & \vdots \\
            0 & \cdots & R_{\vert F \vert} - \frac{1}{\vert F\vert^2}
            \end{bmatrix}
        \]

        More specifically, each $B_i = R_i - \frac{1}{\vert F\vert^2}$ is of the
        form 

        \[ B_i = R_i - \frac{1}{\vert F\vert^2} = \begin{bmatrix}
            \frac{1}{\vert F\vert} - \frac{1}{\vert F\vert^2} & \cdots & - \frac{1}{\vert F\vert^2} \\
            \vdots & \ddots & \vdots \\
            -\frac{1}{\vert F\vert^2} & \cdots & \frac{1}{\vert F\vert} - \frac{1}{\vert F\vert^2}
        \end{bmatrix} \]

        Since $E$ only contains non-zero entries along the diagonal blocks, its
        largest eigenvector is the largest among that of $R_i - S_i$.

        Let $B_i = R_i - S_i$ be one of the diagonal blocks, to analyze the
        eigenvector of $B_i$, we observe that for $v_1 = \mathds{1}$, we have
        \[\begin{array}{rll}
            B_i v_1 &\overset{(1)}{=}& (\frac{1}{\vert F\vert} - \frac{1}{\vert F\vert^2}) \mathds{1} - \frac{1}{\vert F\vert^2} (\vert F\vert - 1) \mathds{1} \\
            &=& (\frac{1}{\vert F\vert} - \frac{1}{\vert F\vert^2} - \frac{\vert F\vert - 1}{\vert F\vert^2}) \mathds{1} \\
            &=& (\frac{\vert F\vert - 1 - (\vert F\vert - 1)}{\vert F\vert^2}) \mathds{1} \\
            &=& 0 \\
            &=& 0v_1 \\
        \end{array} \]

        where in (1), we sum up each column of $B_i$, and observe that on each
        row, there is exactly one entry with value $\frac{1}{\vert
        F\vert}-\frac{1}{\vert F\vert^2}$, and $\vert F\vert-1$ entries in each
        row with value $\frac{1}{\vert F\vert^2}$.

        Thus $v_1 = \mathds{1}$ is an eigenvector of $B_i$ with eigenvalue $0$.

        We now observe that for any $v_2$ such that $v_1 \perp v_2$ ($\langle
        v_1,v_2 \rangle = 0$), we have
        \[ \begin{array}{rll}
            B_i v_2 &=& R_iv_2 - \frac{1}{\vert F\vert^2} (v_1 v_1^T) v_2 \\
            &=& R_iv_2 - \frac{1}{\vert F\vert^2} v_1 (v_1^T v_2) \\
            &=& \frac{1}{\vert F\vert}v_2 - \frac{1}{\vert F\vert^2}v_1\langle v_1,v_2 \rangle \\
            &\overset{(2)}{=}& \frac{1}{\vert F\vert}v_2 \\
        \end{array} \]
        where (2) is because $\langle v_1, v_2 \rangle = 0$.

        Thus $v_2$ is an eigenvector of $B_i$ with eigenvalue $\frac{1}{\vert F\vert}$.

        We thus established that all vectors perpendicular to $\mathds{1}$ are
        eigenvectors of $B_i$ with eigenvalue $\frac{1}{\vert F\vert}$.

        Therefore, the maximum eigenvalue of $B_i$ is $\frac{1}{\vert F\vert}$.

        Thus the second largest eigenvalue of $G^2$ is at most
        $\lambda(G^2) \le \frac{1}{\vert F\vert}$.

    \end{proof}

    \item 
    \begin{proof}

        We can multiply $G^2$ with itself 4 times, that is $G^2 \times G^2
        \times G^2 \times G^2$, which would give us a $(\vert F\vert^8, \cdots,
        \cdots)$

        TODO:
        zig-zagging helps maintain constant degree
    \end{proof}

\end{enumerate}

\newpage
\section*{Problem 4.2}

\begin{enumerate}
    \item \begin{proof}
        For an independent set of vertices $S \subseteq G$, we have

        $$\mathds{1}_S^T R \mathds{1}_S = 0$$

        We split $\mathds{1}_S = v_1 + v_2$ s.t. $v_1 = \frac{\langle
        \mathds{1}_S, \mathds{1}\rangle}{\langle \mathds{1}, \mathds{1}\rangle}
        \mathds{1}$ and $v_1 \perp v_2$.

        Substituting, we have 
        \[\begin{array}{lll}
            \mathds{1}_S^T R \mathds{1}_S \\
            &=& (v_1 + v_2)^T R (v_1 + v_2) \\
            &\overset{(1)}{=}& (v_1^T R v_1) + (v_2^T R v_2) \\
            &\overset{(2)}{=}& \langle v_1, v_1 \rangle + \langle v_2, R v_2 \rangle \\
            &=& 0
        \end{array}
        \]

        where (1) is because $v_1 \perp v_2$, and cross terms cancels out, and
        (2) is because $v_1$ is the eigenvector of $R$ with eigenvalue 1 due to
        regularity.

        Rearranging the above, we have
        $$ \| v_1\|^2 + \langle v_2, R v_2 \rangle = 0$$

        Observe that $\vert \langle v_2, R v_2 \rangle \vert \le \lambda
        \|v_2\|^2$ due to spectral expansion properties, we have
        $$ \| v_1\|^2 = \vert \langle v_2, R v_2 \rangle \vert \le \lambda \|v_2\|^2$$

        We also have $v_1 = \begin{bmatrix}
            \vert S\vert/N \\ \vdots \\ \vert S\vert/N \\ 
        \end{bmatrix}$ and thus
        $$\| v_1\|^2 = (\vert S\vert/N)^2 N = \vert S\vert^2/N$$

        We also have $$\| v_2 \|^2 = \| \mathds{1}_S \|^2 - \| v_1 \|^2 = \vert
        S\vert - \vert S\vert^2/N$$

        Substituting the quantities into the previous inequality, we get 
        $$ \vert S\vert^2/N \le \lambda (\vert S\vert - \vert S\vert^2/N)$$

        Solving the equation, we have $$\vert S\vert \le \frac{\lambda}{\lambda
        + 1}N$$

        \end{proof}
    \item \begin{proof}

        Observe that for a properly colored graph, all vertices of the same
        color must an independent set. Suppose the graph can be colored by less
        than $\frac{1+\lambda}{\lambda}$ colors, then there must be less than
        $\frac{1+\lambda}{\lambda}$ independent sets, which means the average
        size of an independent set is greater than $N\frac{\lambda}{1+\lambda}$,
        which implies there is at least one independent set $S$ such that $\vert
        S \vert > N\frac{\lambda}{1+\lambda}$, which is a contradiction
        according to the previous question.
    \end{proof}

    \item \begin{proof}

        We simulate a random walk on the graph, starting with an initial
        distribution $s_0 = \mathds{1}_u$ where $u$ is the starting vertex.

        The stationary distribution of the graph is $s = \begin{bmatrix}
            1/N \\ 1/N \\ \vdots \\ 1/N
        \end{bmatrix}$.

        By theorem 21.11, we have that $\| R^ks_0 - s \| \le (1 - \gamma)^k
        \sqrt{\Delta_{max}/\Delta_{min}}$.
        
        Since we have a regular graph, $\Delta_{max} = \Delta_{min}$, and
        $\sqrt{\Delta_{max}/\Delta_{min}} = 1$.
        
        Thus $\| R^ks_0 - s \| =  \| R^ks_0 - \frac{1}{N} \| \le (1 - \gamma)^k = \lambda^k$.

        Observe that for some constant $C$, the graph has diameter $C k$ iff
        $R^ks_0$ has no zero entries, which means every vertex must have a
        non-zero probability of being explored after $k$ steps.

        Given $\| R^ks_0 - \frac{1}{N} \| \le \lambda^k$, we also know that the
        point-wise difference is at most the $\ell_2$ norm of the error:

        $$|(R^k s_0)_v - \frac{1}{N}| \le \| R^k s_0 - \frac{1}{N} \| \le \lambda^k$$
        
        % the worst case
        % scenario is $$(R^ks_0)_v - \frac{1}{N} = -\lambda^k$$ so that $\| R^ks_0
        % - \frac{1}{N} \| \le \lambda^k$ still holds, and $$\sum_{i \ne
        % v}(R^ks_0)_i = 1 - \frac{1}{N} + \lambda^k$$ so that probabilities sum
        % to 1. 

        For $(R^ks_0)_v > 0$ to be true, the worst case scenario is when
        $(R^ks_0)_v - \frac{1}{N} = -\lambda^k$, in which case we must have $0 <
        \frac{1}{N} - \lambda^k$, thus 

        $$\lambda^k < \frac{1}{N}$$
        $$k \log \lambda < \log \frac{1}{N}$$
        $$k > -\frac{\log N}{\log \lambda}$$
        $$k > {\log_{1/\lambda} N}$$

        Thus, we have shown that a random walk takes at most $k >
        {\log_{1/\lambda} N} = O({\log_{1/\lambda} N})$ steps to visit every
        node, which is an upper bound for the diameter of the graph.
        

    \end{proof}



\end{enumerate}

\end{document}