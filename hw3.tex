\documentclass[12pt]{article}
\usepackage{amsmath, amssymb}
\newcommand{\pts}[1]{\textbf{[#1 pts]}}

\title{CS 588: Homework 3}
\author{David Deng}

\begin{document}
\maketitle

\section*{Exercise 9.3}

Let $u$, $v$ be unit vectors such that $au = x$ and $bv = y$ for some $a,b \in \mathbb{R}^+$.
To show 
$$\vert\langle f(x), f(y) \rangle - \langle x,y\rangle \vert \leq \epsilon \|x\|\|y\|$$
it suffices to show
$$\vert\langle f(au), f(bv) \rangle - \langle au,bv\rangle \vert \leq \epsilon ab\|u\|\|v\|$$
Since $f$ is linear, $f(au) = af(u)$ and $f(av) = af(v)$. By property of dot product, we also have $\langle ax, by\rangle = ab\langle x, y\rangle$.
Thus, the above is also equivalent to
$$\vert ab \langle f(u), f(v) \rangle - ab \langle u,v\rangle \vert \leq \epsilon ab\|u\|\|v\|$$
Since $a,b > 0$, and unit vector $\|u\| = \|v\| = 1$, we have
$$ab \vert \langle f(u), f(v) \rangle - \langle u,v\rangle \vert \leq \epsilon ab\|u\|\|v\| = \epsilon$$
and thus showing
$$\vert \langle f(u), f(v) \rangle - \langle u,v\rangle \vert \leq \epsilon$$
is enough to solve this question.

Recall for any vector $x$ and $y$, we have 
$$\|x + y \|^2 = \sum_{i=1}^d (x_i + y_i)^2 = \sum_{i=1}^d x_i^2 + y_i^2 + 2x_iy_i = \|x\|^2 + \|y\|^2 + 2\langle x,y\rangle$$
Similarly,
$$\|x - y \|^2 = \sum_{i=1}^d (x_i - y_i)^2 = \sum_{i=1}^d x_i^2 + y_i^2 - 2x_iy_i = \|x\|^2 + \|y\|^2 - 2\langle x,y\rangle$$
Subtracting the two equations, we have
$$\|x + y \|^2 - \|x - y \|^2 = 4\langle x,y\rangle$$
and thus
$$\langle x,y\rangle = \frac{\|x + y \|^2 - \|x - y \|^2}{4}$$
Using this property, we can simplify the LHS of the equation further:
Let $U = \|f(u)+f(v)\|^2-\|f(u)-f(v)\|^2$, and $V = \|u+v\|^2 - \|u-v\|^2$, 
and let $U' = \|f(u)+f(v)\|^2-\|u+v\|^2$, and $V' = \|f(u)-f(v)\|^2- \|u-v\|^2$, 

We have $U-V = U'-V'$.
\begin{align*}
\vert \langle f(u), f(v) \rangle - \langle u,v\rangle \vert = & \vert\frac{U-V}{4}\vert\\
=& \vert\frac{U'-V'}{4}\vert \\
\leq & \frac{\vert U'\vert + \vert V'\vert}{4} \\
\overset{(a)}{\leq}& \frac{\epsilon (\|u+v\|)^2 + \epsilon (\|u-v\|)^2}{4} \\
=& \frac{2\epsilon (\|u\|^2 + \|v\|^2)}{4} \\
=& \epsilon
\end{align*}
where (a) is by JL, where $\vert U' \vert= \vert \|f(u)+f(v)\|^2-\|u+v\|^2 \vert \leq \epsilon (\|u+v\|)^2$, and $\vert V' \vert= \vert \|f(u)-f(v)\|^2-\|u-v\|^2 \vert \leq \epsilon (\|u-v\|)^2$.

\section*{Exercise 9.4}

\begin{enumerate}
\item We prove this by contradiction.
First, suppose $N$ is not an $\epsilon$-net, then there is some point $s\in\mathbb{S}^k$, such that $\|s-x\| > \epsilon$ for all $x \in N$.
Then we could insert $s$ into $N$, and $N \cup \{s\}$ would satisfy the invariant that the distance between any two points $> \epsilon$. However, since $N$ is declared to be a maximal set, no such point $s$ could exist. Therefore, $N$ must be an $\epsilon$-net.

Second, suppose there are more than $(1 + \frac{2}{\epsilon})^k$ points in $N$. Let $V$ be the total volume of the packed spheres, and let $V'$ be the volume of the outer sphere of radius $1+\epsilon/2$. Then we have
$$V' > V > (1+\frac{2}{\epsilon})^k \cdot c_k r^k = (1+\frac{2}{\epsilon})^k \cdot c_k (\epsilon/2)^k = c_k (\frac{2+\epsilon}{\epsilon}\cdot \frac{\epsilon}{2})^k = c_k(1+\frac{\epsilon}{2})^k = V'$$
which is a contradiction.

\item Let $x, y \in N$, by definition of subspace, $x,y \in \mathbb{R}^n$ . Since $P \sim \mathcal{N}^{l\times n}$ is a gaussian projection matrix, by JL, for any target dimension $l = O(\log n/\epsilon^2)$, we have
$$\Pr[\|Px\|^2 - \|x\|^2 \ge \epsilon] \leq 2e^{-O(\epsilon^2\ell)}$$
And by Exercise 9.3, we have that w.h.p.,
$$\vert\langle f(x), f(y) \rangle - \langle x,y\rangle \vert \leq \epsilon \|x\|\|y\|$$
With $\epsilon = 1/2$, we have 
$$\Pr[\|Px\|^2 - \|x\|^2 \le 1/2] \leq 1 - 2e^{-O(\epsilon^2\ell)}$$
 and w.h.p,
 $$\vert\langle f(x), f(y) \rangle - \langle x,y\rangle \vert \leq 1/2 \|x\|\|y\|$$
Let $\epsilon = 1/2$ and $\ell=O(k/\epsilon^2)$, we have 
$$\Pr[\|Px\|^2 - \|x\|^2 \ge 1/2] \leq e^{-O(k/4)}$$
By union bound, the probability that any of the $5^k$ points in $\mathbb{S}^k$ have error larger than $1/2$ is
$$\Pr[\|Px\|^2 - \|x\|^2 \ge 1/2] \leq \sum_x e^{-O(k/4)} = 5^k e^{-O(k/4)} = e^{(\log 5)k - k/4}$$
\item Choose $x_0$ to be the closest point in $N$ to $x$. $\|x-x_0\| \leq 1/2$ becuase $N$ is a $1/2$-net.
Choose $\frac{x_1}{\|x-x_0\|}$ to be the closest point in $N$ to $\frac{x-x_0}{\|x-x_0\|}$ and let $\|x_1\| = \|x-x_0\| \leq 1/2$.
We now have, by $1/2$-net, 
$$\|\frac{x_1-(x-x_0)}{\|x-x_0\|}\| \leq 1/2$$
Since $\|x-x_0\| \leq 1/2$, $\|x_1-(x-x_0)\| = \|x-x_0-x_1\| \leq 1/4$.

In general, let $\|x_i\| = \|x - \sum_{j=0}^{i-1} x_j\|\leq 2^{-i}$, and pick its direction to be the point in $N$ that is closest to $\frac{x-\sum_{j=0}^{i-1} x_j}{\|x-\sum_{j=0}^{i-1} x_j\|}$, then we have both (a) and (b) by definition.

\item Following step 1, build maximal set of points $N_1, N_2, N_3, \cdots$, where $N_i$ is an $2^{-i}$-net, and that $N \subseteq N_1 \subseteq N_2 \subseteq \cdots$ . By step 3, we can always express some unit vector $x$ in $\mathbb{S}^k$ as $x = x_0 + x_1 + x_2 + \cdots$, where $\| x_i \| \leq 2^{-i}$, thus we can union bound the projection error of some arbitrary $x \in \mathbb{S}^k$:

TODO:
$$\|Px\|^2 - \|x\|^2 = \cdots = \sum_i2^{-i} = 1 $$

By $\epsilon$-net property, each $x_i$ has a point $y_i$ in its corresponding $N_i$ set such that $\|y_i-x_i\| \leq 2^{-i}$.

Since $\mathbb{S}^k$ is a unit sphere in the k-dimensional space $U$, by scaling, this bound also holds for all vectors in $U$.
\end{enumerate}

\section*{Exercise 10.1}
\begin{enumerate}
    \item 
$$\Pr[h(s) = h(t)] = 1 - \text{Hamming}(s,t)$$
\item
Let $k$ and $l$ be parameters to be determined.

Build a $k$-ary hash function $h(s) = (h_1(s),\cdots,h_k(s))$, allocating $2^k$ buckets corresponding to the $k$-bit hash signature $h(s) \in \{0,1\}^k$ for each point.

To query a point $s$, we compute $h(s)$, and if the corresponding bucket is non-empty, we iterate through the bucket and return the first valid point.

By 1, we note that for any point $s_2 \in P(h(s))$, the probability of returning a "far" point is
$$\Pr[h(s) = h(s_2) \ \vert\ \text{Hamming}(s,s_2) \geq 2r]  \leq (1-2r)^k$$
The probability of not returning a "near" point is
$$\Pr[h(s) \neq h(s_2) \ \vert\ \text{Hamming}(s,s_2) \leq r]  \leq 1 - (1-r)^k$$
Duplicate this data structure $l$ times, and return a point if any of them returns a valid point.

Then probability of getting "far" point becomes $l \cdot (1-2r)^k$
The probability of returning "near" point becomes $1-(1-(1-r)^k)^l$

let $k=\cdots$, and $l = \cdots$ 

Preprocessing data takes $O(nkl)$ time.
The expected number of bad points we get is $l \cdot(1-2r)^k$

\item
To use the data structure, we first hash all points into the buckets, and to query a point, we look into the corresponding bucket to see if any of the $l$ copies are non-emtpy. We iterate through the entries and return the first valid point.

\section*{Exercise 11.4}
We perform the same $L_1$-metric embedding as before, but instead of setting $\delta$ to 1/Poly(n), we set $\delta = 1/k^C$ for some constant $C$.

This way, theroem 11.7 gives the deterministic upper bound $\|f(u)-f(v)\|_1 \leq O(\log(k))d(u,v)$ for some pair $(u,v)$.

And we have
$$\Pr[\|f(u)-f(v)\|_1 \leq c\cdot d(u,v)] \leq \delta = 1/k^C$$
for some $0 < c < 1$.

Taking a union bound over all $k$ input pairs, we have
$$\Pr[\|f(u)-f(v)\|_1 \leq d(u,v)] \leq \delta = k/k^C = 1/poly(k)$$
which is a high probability with some choice of $C$.
\end{enumerate}

\section*{Exercise 12.6}

\begin{enumerate}
\item Find the common ancestor, and replace each tree edge by the supporting shortest path in the graph $G$.
\item For a solution in G from $s$ to $t$, we map to the tree by tracing up to the common ancestor and back. 
The cost of the solution in $T$ can be a factor of $\log n$ more than the cost of solution in $G$, because at each level $i$, the expected contribution is constant, and there are at most $\log n$ such levels.
\item Since we are building auxiliary edges for the tree, the solution in G will never have a cost more than the cost in T.
\item The tree metric will never have less cost than the graph metric. And since the tree metric overapproximates by $\log n$, it has expected cost of $O(\log n)$ OPT.
\end{enumerate}

\section*{CS588 HW3 \quad Self-Grade}

\subsection*{Score Summary}
\begin{itemize}
  \item Exercise 9.3: \textbf{10 / 10}
  \item Exercise 9.4: \textbf{8 / 10}
  \item Exercise 10.1: \textbf{7 / 10}
  \item Exercise 11.4: \textbf{4 / 10}
  \item Exercise 12.6: \textbf{6 / 10}
\end{itemize}
\noindent \textbf{Total: 35 / 50}

\bigskip
\section*{Per-Problem Notes (Rubric Mapping)}

\subsection*{Exercise 9.3 \quad \pts{10} \quad \emph{Score: 10}}
\textbf{What I did right.}
I reduced to unit vectors, used linearity of the JL map to scale back to general $x=au$, $y=bv$, and applied the polarization identity
\[
\langle f(u),f(v)\rangle-\langle u,v\rangle
=\tfrac14\!\left(\|f(u){+}f(v)\|^2-\|u{+}v\|^2\right)
-\tfrac14\!\left(\|f(u){-}f(v)\|^2-\|u{-}v\|^2\right),
\]
then bounded each squared-norm error by $\varepsilon$ to conclude the desired $O(\varepsilon)$ distortion for unit vectors and hence $\varepsilon\|x\|\|y\|$ in general. \emph{Full credit.}

\subsection*{Exercise 9.4 \quad \pts{10} \quad \emph{Score: 8}}
\textbf{What the rubric expects (high level).}
(i) Build a maximal $\varepsilon$-net and bound its size; (ii) apply JL on the net with a union bound to set the target dimension $\ell=O(\varepsilon^{-2}\log|N|)$; (iii) write any unit vector as a geometric sum of scaled net points; (iv) telescope errors to all vectors.

\textbf{Credit I'm taking.}
I proved the $\varepsilon$-net maximality/packing argument (good), set up JL+$\,$union bound on the net (good, though my tail constants were a bit loose), and constructed the geometric decomposition $x=\sum_i x_i$ with $\|x_i\|\le 2^{-i}$ and nearest-net alignment.

\textbf{Where I fell short.}
I left the final telescoping/error-assembly step as a TODO and didn't cleanly pin the $\ell$ constant to beat the union bound with explicit confidence. \emph{partial: 8/10.}

\subsection*{Exercise 10.1 \quad \pts{10} \quad \emph{Score: 7}}
\textbf{What the rubric expects (high level).}
Standard LSH amplification for Hamming: define $k$-bit signatures so near pairs collide w.p. $\ge (1-r)^k$ and far pairs with $\le (1-2r)^k$; replicate $L$ tables; choose $(k,L)$ so (i) recall is high and (ii) false positives per table are small; give build/query bounds.

\textbf{Credit I'm taking.}
I correctly wrote base collision probabilities, built the $k$-bit hash, replicated $L$ structures, derived: far-hit $\le L(1-2r)^k$ and miss-near $\le (1-(1-r)^k)^L$, and noted preprocessing/query effects.

\textbf{Where I fell short.}
I left $k$ and $L$ as “$\cdots$” and didn't finalize the asymptotically correct choices (e.g., $k=\Theta(\log n)$, $L=n^\rho$ for the right $\rho$), nor did I fully spell out the expected query time bound in terms of $n^\rho$. \emph{partial: 7/10.}

\subsection*{Exercise 11.4 \quad \pts{10} \quad \emph{Score: 4}}
\textbf{What the rubric expects (high level).}
For $k$ terminal pairs, restrict the embedding/sampling to the $k$ terminals and prove an $O(\log k)$ distortion (e.g., via Fréchet/FRT-style coordinates calibrated to terminals), then apply it to the terminal-pairs objective.

\textbf{Credit I'm taking.}
I recognized switching to an $L_1$-type embedding and tuning failure probability to $1/\mathrm{poly}(k)$ to union-bound over $k$ pairs.

\textbf{Where I fell short.}
I mixed up the direction/constants of the distortion guarantee and didn't give the actual $O(\log k)$ terminal-restricted distortion argument; my probability inequalities were off and I didn't connect the embedding to the target sparsest-cut/terminal objective rigorously. \emph{Light partial: 4/10.}

\subsection*{Exercise 12.6 \quad \pts{10} \quad \emph{Score: 6}}
\textbf{What the rubric expects (high level).}
Solve the buy-at-bulk (or related) on a random tree metric $T$ (FRT), use monotone/subadditive $f$, compare costs $G\!\leftrightarrow\!T$ using expected $O(\log n)$ stretch, map back by replacing tree edges with shortest paths and invoke subadditivity; conclude an $O(\log n)$-approximation in expectation.

\textbf{Credit I'm taking.}
I outlined the common-ancestor routing on $T$, stated the $O(\log n)$-level decomposition intuition, mentioned the $G\to T$ and $T\to G$ mappings and that subadditivity controls aggregate costs.

\textbf{Where I fell short.}
I didn't explicitly take expectation over the random tree, nor write the inequality chain that uses subadditivity to bound the back-mapped capacity/cost on $G$ and close the $O(\log n)$ factor. \emph{Solid partial: 6/10.}

\end{document}