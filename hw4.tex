\documentclass[12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage[ruled,vlined,linesnumbered,noend]{algorithm2e}
\usepackage{dsfont}
\usepackage{fancyhdr}

\newcommand{\hw}{CS 588: Homework 4}
\newcommand{\me}{David Deng}

\pagestyle{fancy}

\fancyhf{} % clear all header/footer fields

% Optional thin rule above the footer (comment out if you don't want it)
\renewcommand{\footrulewidth}{0.4pt}

\fancyfoot[L]{\hw}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\me}

% Ensure chapter/section opening pages also use the same footer (for classes that use 'plain' style)
\fancypagestyle{plain}{%
%   \fancyhf{}
%   \renewcommand{\headrulewidth}{0pt}
%   \renewcommand{\footrulewidth}{0.4pt}
%   \fancyfoot[L]{David Deng}
%   \fancyfoot[R]{\hw}
\fancyfoot[L]{\hw}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\me}
}

% \title{\hw}
% \author{David Deng}


\begin{document}
% \maketitle

\section*{Question 13.2}

Let $Q_1$ and $Q_2$ uniformly sample $k$ points each, with $k$ TBD.

Recall the following definition:
$$ A \overset{\text{def}}{=} \text{the event that } Q_1 \cap r = \emptyset \text{ and } \mu(r) \ge \epsilon \text{ for some } r \in R$$
$$ B \overset{\text{def}}{=} \text{the event that } Q_1 \cap r = \emptyset \text{ and } \mu_2(r) > \epsilon/2 \text{ for some } r \in R$$


Define $C$ as the event that 
$$Q_1 \cap r = \emptyset \text{ and } \mu(r) \ge \epsilon \text{ and } \mu_2(r) > \epsilon/2 \text{ for some } r \in R$$

We want to show $\Pr[A] \le \delta$.  Observe that it suffices to show
\begin{enumerate}
    \item $\Pr[B] \ge \frac{\Pr[A]}{2}$ and
    \item $\Pr[B] \le \frac{\delta}{2}$
\end{enumerate}

To show (1), we observe that similar to in the $\epsilon$-sample theorem, we
have $\Pr[B] \ge \Pr[C] = \Pr[C,A] = \Pr[C\vert A]\Pr[A]$, where it suffices to
show $\Pr[C\vert A] \ge 1/2$.
Since we have by event $A$ that $\mu(r) \ge \epsilon$, and since sample $Q_2$ is
the average sum of independent Bernoulli variables $\frac{1}{k}\sum_{i}^{k}X_i$ where $$X_i
= \mathds{1}[\text{the }i^{th} \text{ sample is in } r]$$ 
We can apply additive Chernoff bound: $\Pr[\mu_2(r) < \epsilon/2] \le e^{-\epsilon^2k} \le 1/2$ for $k > \log 2/\epsilon^2$.
Since $Q_1$ and $Q_2$ are independent samples, $Q_1 \cap r = \emptyset$ is irrelevant in the analysis of $Q_2$, therefore
$\Pr[C\vert A] = \Pr[C\vert \mu(r) \ge \epsilon] \ge 1/2$.

To show (2), we imagine sampling $Q_0$ with $2k$ points, and randomly split them
into two sets $Q_1$ and $Q_2$, of size $k$ each. Observe that $P[B] =
\sum_{Q_0} P[B\vert Q_0]P[Q_0] \le \max_{Q_0}P[B\vert Q_0]$, we fix some $Q_0$,
and it suffices to show that for some arbitrary $Q_0$, we have $P[B] \le
\frac{\delta}{2}$.

Since we have $\mu_2(r) > \epsilon/2$, we have $\mu_0(r) > \epsilon/4$.
There are at least $2k\epsilon/4 = k\epsilon/2$ points in $r$.

Let $X_i$ be the event that some random sample from $Q_0$ is in $r$, then
$\Pr[X_i] > \epsilon/4$, and $\Pr[\sum_{i}^{k} \mathds{1}[\text{the } i^{th}
\text{ sample is in }Q_0] = 0] < (1-\epsilon/4)^k < e^{-k\epsilon/4}$.


Union bound with up to $g(2k)$ unique ranges, we have
$$\Pr[B] \le g(2k)e^{-k\epsilon/4}$$

Choose $k > 8\log(g(2k)/\delta)/\epsilon$, we have $\Pr[B] \le \delta/2$.


% Thus, choose $k > \cdots$,

% we have $\Pr[\vert \mu_1(r) - \mu_0(r)\vert + \vert \mu_2(r) - \mu_0(r)\vert >
% \epsilon/2] \le \delta/2$.

% Idea: We only care about the ``heavy'' ranges, by the $\epsilon$-net theorem statement, where $\mu(r) \ge \epsilon$.

% So in analyzing $\Pr[B]$, we know that it only occurs if $\mu_2(r) \ge \epsilon/2$, which means $\mu_0(r) \ge \epsilon/4$.

% Let $D \overset{\text{def}}{=}\vert \mu_1(r) - \mu_2(r)\vert > \epsilon/2$.
% We observe $B$ implies $D$, thus $\Pr[B] \le \Pr[D]$. 
% By triangle inequality, we have $\vert \mu_1(r) - \mu_2(r)\vert \le \vert \mu_1(r) - \mu_0(r)\vert + \vert \mu_2(r) - \mu_0(r)\vert$.
% Therefore $$\Pr[B] \le \Pr[D] \le \Pr[\vert \mu_1(r) - \mu_0(r)\vert + \vert \mu_2(r) - \mu_0(r)\vert > \epsilon/2]$$
% and it suffices to show $\Pr[\vert \mu_1(r) - \mu_0(r)\vert + \vert \mu_2(r) - \mu_0(r)\vert > \epsilon/2] \le \delta/2$.


% For a particular range $r$, by additive Chernoff bound, we have 
% $$\Pr[\vert \mu_1(r) - \mu_0(r) \vert\ge \epsilon/4] \le e^{-k\epsilon^2/8}$$
% and
% $$\Pr[\vert \mu_2(r) - \mu_0(r) \vert\ge \epsilon/4] \le e^{-k\epsilon^2/8}$$

% Observe for the restricted space $Q_0$, there are at most $g(2k)$ distinct
% ranges.  So by union bound, for \textit{all} ranges $r$, we have
% $$\Pr[\vert \mu_1(r) - \mu_0(r) \vert\ge \epsilon/4] \le g(2k)e^{-k\epsilon^2/8}$$
% and
% $$\Pr[\vert \mu_2(r) - \mu_0(r) \vert\ge \epsilon/4] \le g(2k)e^{-k\epsilon^2/8}$$

\newpage

\section*{Question 13.3}

We use the following algorithm to find an $O(\log k)$-approximation for the minimum cardinality hitting set:

\newcommand{\marg}[2]{f(#1 \mid #2)}

\begin{algorithm}[H]
\caption{\textsc{Randomized-Min-Hitting-Set}\(k,D,P\)}
\DontPrintSemicolon
\KwIn{$k \in \mathbb{N}$: an input parameter, 
$D=\{d_1,\cdots,d_m\subseteq \mathbb{R}^2\}$: the set of disks, 
$P=\{p_1,\cdots,p_n\in\mathbb{R}^2\}$: the set of points}
\KwOut{set \(Q \subseteq P\) which hits all disks in $D$}
Let $w$ be a vector in $\mathbb{R}^n$\;
$w \gets [1,\cdots,1]$\;
\For{1 \KwTo t}{
    sample $k\log k$ points $Q \subseteq P$ with distribution $w$\;
    \If{$Q$ hits all $D_i \in D$}{
        \Return $Q$\;
    }
    pick a $D_i$ not hit by $Q$\;
    \For{each point $p_j \in D$}{
        $w(j) \gets 2w(j)$\; 
    }
}
\Return fail\;
\end{algorithm}

Let OPT be the size of the minimum hitting set for $D$ in $P$, 

We first show
that for appropriate $k \approx OPT$, the algorithm always returns a hitting set
for some appropriate $t$, and we get a $\log k$-approximate hitting set.

We can perform a binary search by running at most $\log k$ iterations of the
algorithm to obtain the appropriate measure of $k$.

Thus, we only need to show that with an appropriate measure $k \approx OPT$, the
algorithm terminates with a hitting set.

Let $q = OPT$, and let $Q^*$ be the minimum hitting set such that $\vert
Q^*\vert = q$. Let $\Phi_T = \prod_{p\in Q^*} w_T(p)$ be a measure of
``potential'' on the set $Q^*$ after the $T^{th}$ round of the algorithm for
some $T<t$, where $w_T(p)$ is the weight on $p$ at round $T$.

We have $\Phi_0 = 1$.

By definition, $Q^*$ is a hitting set of $D$, so in every iteration of the
algorithm, at least one point in $Q^*$ must have its weight doubled.

Thus we can have a lower bound that $\Phi_T \ge 2^T\Phi_0 = 2^T$.

Since $\Phi_T$ is a product, we can use AM-GM to upper bound it:

$$\Phi_T = \prod_{p\in Q^*} w_T(p) < (\frac{\sum_{p\in Q^*} w_T(p)}{q})^q < (\frac{\sum_{p\in P} w_T(p)}{q})^q \overset{(1)}{\le} (\frac{W_0(1+C/k)^T}{q})^q$$

for some $C$, where $W_T = \sum_{p \in P} w_T(p)$.

(1) is because $\mu(D_i) < 1/k$ for any missed disk $D_i$. We observe that since
disks have a VC-dimension of 3, $g(n) = n^3$, and by $\epsilon$-net theorem, a
sample $Q$ of size $k\log k$ is a $1/k$-net with high probability. Therefore,
any missed disk from the sample must have fewer than $n/k$ points w.h.p.

Now solving the inequality $2^T \le (\frac{W_0(1+C/k)^T}{q})^q$, we have

$$T \le \frac{q(\log W_0 - \log q)}{\log 2 - q\log(1 + C/k)} < \frac{q(\log W_0 - \log q)}{\log 2 - q(1 + C/k)}$$

Setting $k$ to $q$ (due to OPT), and observe that $W_0 = n$, we have 
$$T \le \frac{q(\log (n/q))}{\log 2 - C} \le q(\log(n/q))$$

if we choose some $C < (\log2)/2$.

We now show that we cannot keep increasing the potential for more than
$O(q(log(n/q)))$ rounds, which means the algorithm will return a solution after
at most $O(q(log(n/q)))$ rounds. This completes the proof that the algorithm
terminates with a hitting set with an appropriate measure $k \approx OPT$.


% is because each missed disk tends to be a light disk with less than
% $n\epsilon$ points, so the $\sum{p\in Q^*} w_T(p) \approx W_0(1+C\epsilon)^T$




\newpage

\section*{Question 14.1}

Given a weak PAC learner routine $f$, we propose the following algorithm that
produces a good hypothesis (with error rate $< \epsilon$) with probability at least
$1-\delta$:

\begin{algorithm}[H]
\caption{\textsc{PAC-learner-from-weak-learner}\((f,D)\)}\label{alg:pac-learner}
\DontPrintSemicolon
\KwIn{The weak PAC learner $f$, training dataset $D$}
\KwOut{A PAC learner}
% $D_1 \gets$ sample $m$ points from $D$\;
\For{1 \KwTo k}{
    $D_1,D_2 \gets$ sample two sets of points, each of size $m$\;
    $h \gets $ learn a hypothesis with $f$ with target error rate $\epsilon-t$ on $D_1$\;
    test $h$ on $D_2$\;
    \If{$err_{D_2}(h) \le \epsilon-t/2$}{
        \Return h\;
    }
}
\Return error\;
\end{algorithm}

We show that the above algorithm returns a hypothesis $h$ such that
$\text{err}(h) < \epsilon$ with probability at least $1 - \delta$.

$A \triangleq err(h) < \epsilon - t$ and $B \triangleq err(h) \ge \epsilon - t$.
By definition of a weak PAC learner, in each round, we have $\Pr[A] \ge 1/2$ and
$\Pr[B] < 1/2$.

Observe that our algorithm successfully returns some good hypothesis, as long as
\begin{enumerate}
    \item it does not accept some bad $h$, and 
    \item it does not reject $h$ in \textit{all} $k$ rounds.
\end{enumerate}


\paragraph{We now analyze the first failure event.}
For a fixed round, we have
$$\Pr[\text{accept }h\ \vert\ h \text{ is bad}] = \Pr[err_{D_2}(h) < \epsilon - t/2
\ \vert\ err(h) > \epsilon] \overset{(1)}{<} e^{-t^2m/2} = q$$ where (1) is by
additive Chernoff bound, where $err_{D_2}(h)(h)$ is a sum of independent random
variables and $err(h)$ is the average. We also write $q = e^{-t^2m/2}$.

$\Pr[err(h) > \epsilon] < \Pr[B] < 1/2$ by definition of
weak PAC learner, for any round $$\Pr[\text{accepting }h \land h\text{ is bad}]
\le q/2$$ By union bound, The probability of accepting \textit{any} bad
hypothesis across $k$ rounds is $$\Pr[\text{accepting some bad hypothesis}] \le
kq/2$$

\paragraph{We now analyze the second failure event.}
For a fixed round, 
$$\Pr[\text{reject }h\ \vert\ h \text{ is good}] = \Pr[err_{D_2}(h) > \epsilon -
t/2 \ \vert\ err(h) < \epsilon-t] \overset{(1)}{<} e^{-t^2m/2} = q$$ where (1)
is by additive
Chernoff bound.

Conversely,
$$\Pr[\text{accept }h\ \vert\ h \text{ is good}] = \Pr[err_{D_2}(h) > \epsilon -
t/2 \ \vert\ err(h) < \epsilon-t] \ge 1- q$$
and with $\Pr[A] > 1/2$, we have that for any round,
$$\Pr[\text{accept a good hypothesis}] = \frac{1-q}{2}$$
For $k$ rounds, the chance that none of the rounds yield a 
$$\Pr[\text{reject in all }k \text{ rounds}] = (1 - \frac{1-q}{2})^k \le e^{-(1-q)k/2}$$

Select $m \ge \frac{2\log(k/\delta)}{t^2}$ will give us $q < k\delta$, and the
probability of the first failure event is bounded by $\delta/2$.  Let $k \ge
4\log(2/\delta)$ (assuming conservatively $q \le 1/2$), then the probability of
the second failure event is also bounded by $\delta/2$.  A union bound gives us
the total failure probability of less than $\delta$.

We note that when $\epsilon-t \le err(h) \le \epsilon$, the algorithm can either
accept or reject. This is safe because if it accepts, $h$ is a ``good''
hypothesis since $err(h) < \epsilon$, and if it rejects, the analysis of the
second failure event above demonstrates that such an event is unlikely.

\newpage
\section*{Question 15.3}

\begin{enumerate}

    \item Instead of computing $f$ overall $n$ points, we do it on
    $n\log(1/\epsilon)/k$ elements for each round. So the expected running time
    is $O(n\log(1/\epsilon)/k \cdot kQ) = O(n\log(1/\epsilon)Q)$.

    \item Let $R_i$ be the sample of size $n\log(1/\epsilon)/k$ at round $i$, and let $A_i \subseteq S_{i-1}$ be the top $k$ elements in $S_{i-1}$.
    We analyze the probability that none of the sampled points are the top $k$ elements in $S_{i-1}$, that is,

    $$\Pr[A_i \cap R_i = \emptyset] = (1 - \frac{k}{n})^{n\log(1/\epsilon)/k} \le \epsilon^{\frac{k}{n}\cdot n/k} = \epsilon$$

    Suppose $A_i \cap R_i \neq \emptyset$, then the analysis is the same as the randomized greedy algorithm for a single round. Specifically,

    \begin{align*}
       &\mathbb{E}[\max_{e \in R_i} f(e\vert S_{i-1}) \vert A_i \cap R_i \neq \emptyset] \\
    \overset{(1)}{\ge}\ &\underset{e \in A_i}{\mathbb{E}}[f(e\vert S_{i-1}) \vert A_i \cap R_i \neq \emptyset] \\
    \overset{(2)}{\ge}\ &\underset{e \in S^*}{\mathbb{E}}[f(e\vert S_{i-1}) \vert A_i \cap R_i \neq \emptyset] \\
    \overset{(3)}{\ge}\ &(1-\epsilon) \frac{1}{k} f(S^*\vert S_{i-1}) \\
    \overset{(4)}{\ge}\ & \frac{1-\epsilon}{k} (f(S^*) - f(S_{i-1})) \\
    =\ & \frac{1-\epsilon}{k} (OPT - f(S_{i-1})) 
    \end{align*}

    (1) is because $R_i$ contains at least one element from $A_i$, and it must have be chosen as the maximum.
    (2) is because of randomized greedy choice of $e$, specifically, sampling one from the top $k$ elements is ``better'' than sampling from the overall optimum set $S^*$ (greedy choice of $e$).
    (3) is because of submodularity.
    (4) is because of monotonicity, that $f(S^*) \le f(S^* \cup S_{i-1})$.

    \item Let $q = \frac{1-\epsilon}{k}$, then by the recurrence from the previous question, we have
    \begin{align*}
        \mathbb{E}[f(S_1)] &= q\cdot OPT \\
        \mathbb{E}[f(S_2)] &= q\cdot (OPT - q\cdot OPT) = (q-q^2)\cdot OPT \\
        \mathbb{E}[f(S_3)] &= q\cdot (OPT - (q-q^2)\cdot OPT) = (q-q^2+q^3)\cdot OPT \\
        \cdots
        \mathbb{E}[f(S_i)] &= OPT \sum_{j=1}^i (-1)^{j+1}q^j \\
        &= OPT\cdot q \sum_{j=0}^{i-1} (-q)^j \\
        &= OPT\cdot q \frac{1-(-q)^i}{1+q}
    \end{align*}
    

\end{enumerate}

\newpage
\section*{Question 16.1}

We prove the theorem by constructing an encoding scheme.

According to Shannon's upper bound theorem, for some $\epsilon = \delta^2$,
there is a coding scheme with average error $\le \delta^2$ and transmission rate
$R \ge 1-H(p)-\delta^2$.

Let $\mathcal{C} \in \{0,1\}^m \to \{0,1\}^n$ and $\mathcal{D}\in \{0,1\}^n \to
\{0,1\}^m$ be that coding scheme, and for any string $x \in \{0,1\}^m$, let
$P_x$ denote the event that $x$ will have a transmission error with respect to
this coding scheme. By the theorem, for some string $x$, $\Pr[P_x] \le
\delta^2$, and with $M=2^m$ such strings, the expected error rate is $E =
\frac{1}{M}\underset{x}{\mathbb{E}}[P_x] \le \delta^2$.

By Markov's inequality, $Pr[E > \delta] < \delta$.

This means there are less than $M\delta = 2^m\delta$ strings that have error
w.r.t this coding scheme.

Now we remove those strings, and consider a new encoding space with $m' = \log_2
((1-\delta)2^m)$ and $n' = n$.

We have a new transmission rate

$$R' = \frac{m'}{n'} = \frac{\log_2 (1-\delta) + m}{n} = \frac{m}{n} +
\frac{\log_2 (1-\delta)}{n} \ge 1-H(p)-\delta^2- \frac{\log_2 (1-\delta)}{n}$$

For sufficiently large $n$, this is equal to $1-H(p)-\delta^2 \ge 1-H(p)-\delta$

Thus, we have obtained a coding scheme where all strings have an encoding error
$< \delta$ with the same transmission rate.


\end{document}
